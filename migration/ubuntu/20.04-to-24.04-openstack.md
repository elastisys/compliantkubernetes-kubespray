# Migrate to Ubuntu 24.04 on Openstack/UpCloud

> [!IMPORTANT]
> This migration assumes that the environment is running on an Openstack or UpCloud environment.
> This mainly applies to the terraform details, so you should be able to follow along with the Kubespray steps even if you're not running on an Openstack/UpCloud environment.

## Prerequisites

Before you start you should make sure that if you plan to follow this guide and create all new nodes at the same time, do make sure that you have enough quota for that.
Recommended is to do at least all the control planes for one cluster in one go, but for worker nodes you can replace in smaller groups at the time if it's a large cluster.

> [!WARNING]
> These will completely recreate the nodes, and thus also their IP addresses.
> If the environment depend on these IPs in any way, please communicate this well in advance.

## Prepare

These steps can be done without any disruption to the running cluster.

- Prepare environment variables

  ```bash
  export CK8S_CONFIG_PATH="/path/to/cluster/config"
  export CK8S_KUBESPRAY_REPOSITORY_PATH="/path/to/compliantkubernetes-kubespray"
  export CK8S_APPS_REPOSITORY_PATH="/path/to/compliantkubernetes-apps"
  export KUBECONFIG="${CK8S_CONFIG_PATH}/.state/kube_config_<sc|wc>.yaml"
  ```

- In `${CK8S_CONFIG_PATH}/<sc|wc>-config/cluster.tfvars` update the following:

    - Change global `image_uuid` (Openstack)/`template_name` (UpCloud) to the ID of Ubuntu 24.04
    - Configure `image_id` (Openstack)/`template_name` (UpCloud) for all existing nodes to the old image id

- Verify by running `terraform plan` and make sure there's no changes on any on the existing nodes
- Add a new control plane node that doesn't specify `image_id`/`template_name` (To use the new image)
- Verify by running `terraform plan` and make sure there's a new node added with the correct image
- Add the rest of the new control plane nodes
    - On UpCloud environments, also add the new nodes to `master-api` in `loadbalancers`
- Run `terraform apply` to add all the new nodes
- Backup the current state:

  ```bash
  pushd "${CK8S_CONFIG_PATH}"
  cp -r <sc|wc>-config <sc|wc>-config-backup
  popd
  ```

- Generate new inventory (can be skipped for UpCloud environments):

  ```bash
  ./gen-inventory.sh <sc|wc>
  ```

- Reorder inventory so the old control plane nodes are first in each category they occur in
- Add the new control plane floating IPs into group vars under the key `supplementary_addresses_in_ssl_keys`

> [!NOTE]
> Adding node IPs to `supplementary_addresses_in_ssl_keys` is not needed if you are using a loadbalancer in front of the API server or only use DNS records.
> You can check to see what `supplementary_addresses_in_ssl_keys` is currently pointing to, and if it's not the IPs of the control plane nodes then this can be left alone.

## Execute

These steps will cause disruptions in the cluster.

### Replace control plane nodes

- Add new control plane nodes using Kubespray:

  ```bash
  pushd "${CK8S_KUBESPRAY_REPOSITORY_PATH}"
  ./bin/ck8s-kubespray apply <sc|wc> --limit=etcd,kube_control_plane -b -e=ignore_assert_errors=true --skip-tags=multus
  popd
  ```

- Update control plane proxy using Kubespray:

  ```bash
  pushd "${CK8S_KUBESPRAY_REPOSITORY_PATH}"
  ./bin/ck8s-kubespray apply <sc|wc> --tags=nginx -b
  popd
  ```

- Update the NetworkPolicies:

  ```bash
  pushd "${CK8S_APPS_REPOSITORY_PATH}"
  ./bin/ck8s update-ips <sc|wc> apply

  ./bin/ck8s ops helmfile <sc|wc> -l policy=netpol -i apply
  popd
  ```

- Reorder inventory so the new control plane nodes are first in each category they occur in
- Set the public IP of the first new control plane node into group vars under the key `kube_oidc_apiserver_endpoint`

> [!NOTE]
> Changing `kube_oidc_apiserver_endpoint` is not needed if it's not pointing to an old control plane node directly.
> Like if it's pointing to a loadbalancer or DNS or similar.

- Set internal IP of the first new control plane node into the ConfigMap `kube-public/cluster-info` under the key `server`:

  ```bash
  kubectl edit configmap -n kube-public cluster-info
  ```

- Set internal IP of the first new control plane node into the ConfigMap `kube-system/kubeadm-config` under the key `controlPlaneEndpoint`

  Replace any old control plane hostnames and IP with the new control plane in the same ConfigMap under the key `certSANs`.

  ```bash
  kubectl edit configmap -n kube-system kubeadm-config
  ```

- Update DNS records to point to the new control plane nodes

> [!NOTE]
> Updating the DNS records is only needed if you are using DNS and are pointing that to the IPs of the control plane nodes.
> It could be updating `dns.json` and applying if using AWS

- Update loadbalancer to use the endpoints of the new control plane nodes

> [!NOTE]
> Updating the loadbalancer is only needed if you are using a loadbalancer in front of the API server.

- Update control plane order using Kubespray:

  ```bash
  pushd "${CK8S_KUBESPRAY_REPOSITORY_PATH}"
  ./bin/ck8s-kubespray apply <sc|wc> --tags=master,nginx -b -e=ignore_assert_errors=true
  popd
  ```

- Remove old control plane nodes using Kubespray:

  ```bash
  pushd "${CK8S_KUBESPRAY_REPOSITORY_PATH}"
  ./bin/ck8s-kubespray remove-node <sc|wc> <old-control-plane-nodes,...>
  popd
  ```

> [!NOTE]
> Removing the control plane nodes will likely fail at some point in one or two ways:
>
> - If it fails early for some and succeed for others then remove the removed nodes from inventory and retry.
> - If it fails late for some with timeouts then ignore.

- Check that old etcd members are removed:

  ```bash
  HOSTNAME="$(kubectl get node -l node-role.kubernetes.io/control-plane= -ojsonpath='{.items[0].metadata.name}')"
  kubectl -n kube-system exec -it "etcd-${HOSTNAME}" -- \
    etcdctl --endpoints=https://127.0.0.1:2379 \
            --cacert=/etc/kubernetes/ssl/etcd/ca.crt \
            --cert=/etc/kubernetes/ssl/etcd/server.crt \
            --key=/etc/kubernetes/ssl/etcd/server.key \
            member list
  ```

- If there are old nodes left, remove them using the `uuid` from the previous command (The id in the first column of each row)

  ```bash
  HOSTNAME="$(kubectl get node -l node-role.kubernetes.io/control-plane= -ojsonpath='{.items[0].metadata.name}')"
  kubectl -n kube-system exec -it "etcd-${HOSTNAME}" -- \
    etcdctl --endpoints=https://127.0.0.1:2379 \
            --cacert=/etc/kubernetes/ssl/etcd/ca.crt \
            --cert=/etc/kubernetes/ssl/etcd/server.crt \
            --key=/etc/kubernetes/ssl/etcd/server.key \
            member remove <uuid>
  ```

- Check that old control plane nodes are removed:

  ```bash
  kubectl get nodes
  ```

- If there are any old nodes left, delete them

  ```bash
  kubectl delete nodes <node>,...
  ```

- Change back to using dynamic inventory

  ```bash
  pushd "${CK8S_CONFIG_PATH}"
  cp <sc|wc>-config-backup/inventory.ini <sc|wc>-config/inventory.ini
  popd
  ```

- Remove old control plane nodes using Terraform
- Remove the old control plane floating IPs from group vars under the key `supplementary_addresses_in_ssl_keys`

> [!NOTE]
> Removing the old floating IPs from `supplementary_addresses_in_ssl_keys` is not needed if you are using a loadbalancer in front of the API server.

- Apply kubespray:

  ```bash
  pushd "${CK8S_KUBESPRAY_REPOSITORY_PATH}"
  ./bin/ck8s-kubespray apply <sc|wc> --tags=master,nginx -b
  popd
  ```

- Update the NetworkPolicies:

  ```bash
  pushd "${CK8S_APPS_REPOSITORY_PATH}"
  ./bin/ck8s update-ips <sc|wc> apply

  ./bin/ck8s ops helmfile <sc|wc> -l policy=netpol -i apply
  popd
  ```

### Replace worker nodes

- Add a new worker node that doesn't specify `image_id` (To use the new image)
- Verify by running `terraform plan` and make sure there's a new node added with the correct image
- Add the rest of the new worker nodes
    - On UpCloud environments, also add the new nodes to `http` and `https` in `loadbalancers`
- Run `terraform apply` to add all the new nodes
- Add all the new worker nodes

  ```bash
  pushd "${CK8S_KUBESPRAY_REPOSITORY_PATH}"
  ./bin/ck8s-kubespray run-playbook <sc|wc> facts.yml
  ./bin/ck8s-kubespray run-playbook <sc|wc> scale.yml -b --limit=<new-node1>,<new-node2>,...
  popd
  ```

- Cordon then drain the old nodes and make sure everything starts up properly (Cordon all first to make sure the drain doesn't move things to any old machines)

> [!CAUTION]
> If you are using applications that allowlist the old nodes, please update that list to also include the new nodes before draining the old ones.

  ```bash
  kubectl cordon <old-node1>
  kubectl cordon <old-node2>
  ...
  kubectl drain <old-node1>
  kubectl drain <old-node2>
  ...
  ```

- Remove the old worker nodes

  ```bash
  pushd "${CK8S_KUBESPRAY_REPOSITORY_PATH}"
  ./bin/ck8s-kubespray remove-node <sc|wc> <old-node1>,<old-node2>,...
  popd
  ```

- Remove old worker nodes using Terraform

## Cleanup

- Remove the backed up config folder

  ```bash
  rm -r <sc|wc>-config-backup
  ```
